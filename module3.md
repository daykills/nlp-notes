# Word Embeddings

### Word Embeddingsï¼š A word embedding is a way of representing words as dense numerical vectors so that a machine learning model can understand and work with them.
Key Ideaï¼š 
- Each word is mapped to a vector of real numbers (for example, 100-dimensional).
- Words that appear in similar contexts in text end up with vectors that are close together in this space.
- This means that relationships between words can be learned. For example: vector("king") - vector("man") + vector("woman") â‰ˆ vector("queen").


### Skip-gram
- predict the context words from the target

Defination:  

Input: a single word (the â€œcenterâ€ word).
Output: the probability of each word in the vocabulary being a â€œcontext wordâ€ (a neighbor within a fixed window around the center word).
So instead of predicting the center word from context (CBOW), Skip-gram does the opposite: predicts context words given the center word.

Procedure:

1. Prepare training data
   - Choose a window size k
   - For each word in the corpus, collect all surrounding words within that window.
   - Create training pairs of (center_word, context_word).
2. Input representation
   - Represent the input word as a one-hot vector (size = vocab size).
3. Embedding lookup
   - Multiply the one-hot vector by an embedding matrix W.
   - This selects the row of W that corresponds to the word â†’ the embedding vector.
4. Prediction layer
   - Multiply the embedding by another weight matrix W' to get scores for all vocabulary words.
   - Apply softmax to turn scores into probabilities.
5. Loss function
   - Use cross-entropy loss.
   - For each training pair, maximize the probability of the true context word.
   - Often negative sampling or hierarchical softmax is used for efficiency.
6. Training
   - Repeat for many epochs over the corpus.
   - Adjust weights so that words appearing in similar contexts end up with similar embeddings.
     
Skip-gram Example (tiny corpus)
```

Sentence: "I like cats and dogs"
Vocabulary: [I, like, cats, dogs, and]
Vocab size = 5
Window size = 1

Step 1: Generate training pairs
(I, like)
(like, I), (like, cats)
(cats, like), (cats, and)
(and, cats), (and, dogs)
(dogs, and)
```


### CBOW Model
- The CBOW model predicts a target word given its context words.

```
Example sentence:
"the cat sat on the mat"
If target word = "sat" (index=2)
Context window size k=2
Context words = ["the", "cat", "on", "the"]

Steps:
Convert context words â†’ embeddings.
Average (or sum) the embeddings â†’ get context vector.
Feed into a linear layer â†’ softmax over vocab.
Train model to predict the target word "sat".

ğŸ”¹ Difference from Skip-gram

CBOW: context â†’ predict target
Skip-gram: target â†’ predict context

ğŸ‘‰ Example:

CBOW: â€œthe ___ sat onâ€ â†’ predict â€œcatâ€
Skip-gram: â€œcatâ€ â†’ predict â€œtheâ€, â€œsatâ€
```


### Applying embeddings in the deep networks
- Map from word indices to imbeddings at the first layer of the network
     ```
     å¥å­: "the cat sat"

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   å•è¯è¡¨   â”‚   è¯æ±‡è¡¨å¤§å° V=5
        â”‚ "the": 0   â”‚
        â”‚ "cat": 1   â”‚
        â”‚ "sat": 2   â”‚
        â”‚ "on" : 3   â”‚
        â”‚ "mat": 4   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                   â–¼
   å•è¯ â†’ ç´¢å¼•: ["the", "cat", "sat"] â†’ [0, 1, 2]
                   â”‚
                   â–¼
        One-hot å‘é‡ (é•¿åº¦=V)
        "cat" â†’ [0, 1, 0, 0, 0]
                   â”‚
                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    Embedding çŸ©é˜µ E     â”‚   å¤§å° = (V Ã— d)ï¼Œæ¯”å¦‚ (5 Ã— 3)
    â”‚ "the": [ 0.2, -0.1, 0.5]â”‚
    â”‚ "cat": [-0.3,  0.8, 0.1]â”‚
    â”‚ "sat": [ 0.7,  0.4,-0.6]â”‚
    â”‚ "on" : [-0.2, -0.5, 0.9]â”‚
    â”‚ "mat": [ 0.1,  0.3, 0.7]â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
   Embedding Lookup
   "cat"(ç´¢å¼•=1) â†’ [-0.3, 0.8, 0.1]
   "sat"(ç´¢å¼•=2) â†’ [ 0.7, 0.4,-0.6]
   "the"(ç´¢å¼•=0) â†’ [ 0.2,-0.1, 0.5]
   
   cat" (index=1) ç›´æ¥å–ç¬¬ 1 è¡Œ â†’ [-0.3, 0.8, 0.1]ã€‚
   è¿™æ ·å°±æŠŠè¯å˜æˆäº†ä¸€ä¸ªç¨ å¯†å‘é‡ã€‚
   ---
   
   æœ€ç»ˆå¾—åˆ°çš„å¥å­å‘é‡åºåˆ—:
   è¿™äº›å‘é‡ä¼šé€è¿›åç»­ç½‘ç»œ (RNN, CNN, Transformer ç­‰)ã€‚
   åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œembedding çŸ©é˜µé‡Œçš„è¡Œä¼šæ›´æ–°ï¼Œä»è€Œè®©ç›¸ä¼¼è¯çš„å‘é‡é è¿‘ã€‚
   [ [0.2,-0.1,0.5], [-0.3,0.8,0.1], [0.7,0.4,-0.6] ]
   ```
- Approach 1: Learn these embeddings as parameters from your dataï¼ˆå°±åƒè®©å­¦ç”Ÿä»é›¶å­¦èµ·ï¼Œæ‰€æœ‰çŸ¥è¯†éƒ½è¦è‡ªå·±å­¦ï¼‰
  -  Randomly initialize all parameters and learn with backpropagation.
  -  Can work reasonably well.
- Approach 2: Init word embeddings using GloVe, keep fixed.ï¼ˆå°±åƒç»™å­¦ç”Ÿä¸€æœ¬ç°æˆçš„å­—å…¸ï¼ˆGloVeï¼‰ï¼Œä»–ä»¬ç›´æ¥æœ‰åŸºç¡€çŸ¥è¯†ï¼Œå†åœ¨å…·ä½“ä»»åŠ¡ä¸Šåº”ç”¨ï¼‰
  - Faster because no need to update these parameters.
- Approach 3: initialize word embeddings GloVe, fine-tune
   -  Works best for some tasks
 
### Deep Averaging Networks (DANs) 
   - å‘é‡å–å¹³å‡å‘ç”Ÿåœ¨ æŠŠå•è¯ embedding æ‹¿å‡ºæ¥ä¹‹åã€é€å…¥å…¨è¿æ¥å±‚ä¹‹å‰ï¼Œå‘é‡å–å¹³å‡ä¹‹åï¼Œæ¢¯åº¦ä¼šè¢«å‡åˆ†ï¼Œå†ä¼ å›å»ç»™æ¯ä¸ªå•è¯ embeddingã€‚
   - Instead of using RNNs (which read words sequentially) or CNNs (which look for n-gram patterns), DANs just average the embeddings of all words in the sentence.
   - Then, they pass this averaged vector through one or more feed-forward (fully connected) layers (a â€œdeepâ€ network).
   - Finally, a softmax layer produces a prediction (e.g., sentiment class, topic class).
   



