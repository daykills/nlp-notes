# Word Embeddings

### Word Embeddings： A word embedding is a way of representing words as dense numerical vectors so that a machine learning model can understand and work with them.
Key Idea： 
- Each word is mapped to a vector of real numbers (for example, 100-dimensional).
- Words that appear in similar contexts in text end up with vectors that are close together in this space.
- This means that relationships between words can be learned. For example: vector("king") - vector("man") + vector("woman") ≈ vector("queen").
